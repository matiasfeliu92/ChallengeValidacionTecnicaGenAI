# Data Engineering Challenge: Document Platform ELT Pipeline

Este repositorio contiene una soluci√≥n integral de ingenier√≠a de datos para el procesamiento de logs de una plataforma colaborativa de documentos. El proyecto implementa un pipeline **ELT (Extract, Load, Transform)** utilizando una **Arquitectura de Medall√≥n** (Bronze, Silver, Gold), garantizando la limpieza, integridad y disponibilidad de los datos para anal√≠tica avanzada.



## üöÄ Stack Tecnol√≥gico

* **Lenguaje:** Python 3.8+
* **Validaci√≥n de Datos:** Pydantic
* **Orquestaci√≥n:** Apache Airflow
* **Transformaci√≥n de Datos:** dbt (Data Build Tool) con PostgreSQL
* **Base de Datos:** PostgreSQL
* **Infraestructura:** Docker & Docker Compose
* **Conectividad:** SQLAlchemy

---

## üèóÔ∏è Arquitectura del Pipeline

El flujo de datos se organiza en tres capas incrementales dentro de PostgreSQL:

### 1. Capa Bronze (Raw/Staging)
* **Fuente:** Los logs se extraen de un archivo JSON localizado en `data/raw/mock_event_logs.json`.
* **Validaci√≥n:** Se utiliza **Pydantic** (`src/schemas/parse_json.py`) para asegurar que cada evento cumpla con el esquema esperado antes de la ingesta.
* **Carga:** Los datos se cargan de forma incremental en la tabla `public.raw_event_logs` utilizando una columna de tipo `JSONB` para mantener la flexibilidad del dato original.

### 2. Capa Silver (Normalized)
* **Proceso:** dbt toma los datos de la capa Bronze y los distribuye en tablas relacionales normalizadas.
* **Modelos:** Se generan 4 tablas espec√≠ficas filtradas por tipo de evento:
    * `silver_comments`: Atributos de comentarios.
    * `silver_shares`: Informaci√≥n de documentos compartidos.
    * `silver_edits`: Volumen y detalle de ediciones.
    * `silver_logins`: Registros de inicio de sesi√≥n.
* **Estrategia:** Se utiliza una estrategia de carga **incremental** con `merge` para optimizar el rendimiento y evitar duplicados.

### 3. Capa Gold (Business & Analytics)
* **Proceso:** Consolidaci√≥n de m√©tricas de negocio y KPIs.
* **Tablas Finales:**
    * `daily_active_users (DAU)`: Usuarios √∫nicos activos en los √∫ltimos 30 d√≠as.
    * `average_session_duration_by_user`: Tiempo promedio de sesi√≥n calculado por la diferencia entre el primer y √∫ltimo evento diario.
    * `top_10_most_edited_documents`: Ranking de documentos seg√∫n el volumen acumulado de `edit_length`.
    * `number_of_shared_documents_by_user`: Total de acciones de colaboraci√≥n iniciadas por usuario.
    * `anomaly_analytics`: Detecci√≥n de "Eventos Hu√©rfanos" (acciones realizadas sin un registro de login previo).



---

## üõ†Ô∏è Instrucciones para Montar el Proyecto

Sigue estos pasos para desplegar el entorno completo utilizando Docker:

1.  **Clonar el repositorio:**
    ```bash
    git clone [https://github.com/matiasfeliu92/ChallengeValidacionTecnicaGenAI.git](https://github.com/matiasfeliu92/ChallengeValidacionTecnicaGenAI.git)
    cd ChallengeValidacionTecnicaGenAI
    ```

2.  **Configurar variables de entorno:**
    * Crear un archivo `.env` en la ra√≠z del proyecto.
    * Copiar y adaptar las credenciales del archivo `.env.example`.

3.  **Inicializar la base de datos de Airflow:**
    ```bash
    docker compose up --build airflow-init
    ```

4.  **Levantar los servicios (Airflow, Postgres, dbt):**
    ```bash
    docker compose up --build -d
    ```

5.  **Acceder a la interfaz de Airflow:**
    * URL: `http://localhost:8080`
    * **Usuario:** `admin`
    * **Contrase√±a:** `admin`

6.  **Ejecutar el Pipeline:**
    * Buscar el DAG llamado **`ChallengeValidacionTecnicaGenAI`**.
    * Activar el DAG y ejecutarlo manualmente (Trigger).

---

## üìà An√°lisis de Anomal√≠as
Dentro de la Capa Gold, el modelo `anomaly_analytics` identifica registros inconsistentes donde los usuarios interact√∫an con documentos sin haber disparado un evento de `user_login`. Este an√°lisis permite identificar posibles fallos en la captura de logs de la aplicaci√≥n o latencias en el env√≠o de eventos cr√≠ticos.



---

## üìÇ Estructura del Repositorio
* `/dags`: Definici√≥n del flujo de trabajo en Airflow.
* `/models`: Modelos de dbt para las capas Staging, Silver y Gold.
* `/src`: Scripts de Python para extracci√≥n, carga y validaci√≥n (Pydantic).
* `/data`: Archivos fuente de logs (Raw).

**Desarrollado por:** [Matias Feliu]